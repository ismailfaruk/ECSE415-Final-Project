{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "59e1cae2c9cb520692c884e62f401aba19534860cf0c0ac807756ec6e51c6592"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import imutils\n",
    "def pyramid(image, scale=1.5, minSize=(30, 30)):\n",
    "\t# yield the original image\n",
    "\tyield image\n",
    "\t# keep looping over the pyramid\n",
    "\twhile True:\n",
    "\t\t# compute the new dimensions of the image and resize it\n",
    "\t\tw = int(image.shape[1] / scale)\n",
    "\t\timage = imutils.resize(image, width=w)\n",
    "\t\t# if the resized image does not meet the supplied minimum\n",
    "\t\t# size, then stop constructing the pyramid\n",
    "\t\tif image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "\t\t\tbreak\n",
    "\t\t# yield the next image in the pyramid\n",
    "\t\tyield image\n",
    "def sliding_window(image, stepSize, windowSize):\n",
    "\t# slide a window across the image\n",
    "\tfor y in range(0, image.shape[0], stepSize):\n",
    "\t\tfor x in range(0, image.shape[1], stepSize):\n",
    "\t\t\t# yield the current window\n",
    "\t\t\tyield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "image = cv2.imread(\"frames/seq_000001.jpg\")\n",
    "(winW, winH) = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the image pyramid\n",
    "for resized in pyramid(image, scale=1.5):\n",
    "\t# loop over the sliding window for each layer of the pyramid\n",
    "\tfor (x, y, window) in sliding_window(resized, stepSize=32, windowSize=(winW, winH)):\n",
    "\t\t# if the window does not meet our desired window size, ignore it\n",
    "\t\tif window.shape[0] != winH or window.shape[1] != winW:\n",
    "\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\t# THIS IS WHERE YOU WOULD PROCESS YOUR WINDOW, SUCH AS APPLYING A\n",
    "\t\t# MACHINE LEARNING CLASSIFIER TO CLASSIFY THE CONTENTS OF THE\n",
    "\t\t# WINDOW\n",
    "\t\t# since we do not have a classifier, we'll just draw the window\n",
    "\t\tclone = resized.copy()\n",
    "\t\tcv2.rectangle(clone, (x, y), (x + winW, y + winH), (0, 255, 0), 2)\n",
    "\t\tcv2.imshow(\"Window\", clone)\n",
    "\t\tcv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(58, 23)\n(128, 64)\n(97, 22)\n(128, 64)\n(2, 128, 64)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import random as rg\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "file1 = \"positive\\Image73_person_8.jpg\"\n",
    "file2 = \"positive\\Image73_person_9.jpg\"\n",
    "\n",
    "img1 = cv2.imread(file1, 0)\n",
    "print(img1.shape)\n",
    "img1 = cv2.resize(img1, (64, 128))\n",
    "print(img1.shape)\n",
    "\n",
    "img2 = cv2.imread(file2, 0)\n",
    "print(img2.shape)\n",
    "img2 = cv2.resize(img2, (64, 128))\n",
    "print(img2.shape)\n",
    "\n",
    "R_train_images = np.array([img1, img2])\n",
    "print(R_train_images.shape)\n",
    "\n",
    "positive_path = \"./positive\"\n",
    "negative_path = \"./negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateTrainSet(positive_path, negative_path, IMAGE_WIDTH, IMAGE_HEIGHT, Positive_Images=1200):\n",
    "  # getting all file names from positive path\n",
    "  positives = os.listdir(positive_path)\n",
    "  positive_files = [os.path.join(positive_path, file_name) for file_name in positives if file_name.endswith('.jpg')]\n",
    "  positive_files.sort()\n",
    "\n",
    "  # getting all file names from negative path\n",
    "  negatives = os.listdir(negative_path) \n",
    "  negative_files = [os.path.join(negative_path, file_name) for file_name in negatives if file_name.endswith('.jpg')]\n",
    "  negative_files.sort()\n",
    "\n",
    "  # creating train label np array for pos=0 and neg=1\n",
    "  pos_labels = np.zeros(Positive_Images)\n",
    "  neg_labels = np.ones(len(negative_files))\n",
    "  train_labels = np.concatenate((pos_labels, neg_labels), axis=0).astype(int)\n",
    "  \n",
    "  # add positive images to train_image np array\n",
    "  pos_images = np.zeros((Positive_Images, IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "  for filename in positive_files[0: Positive_Images]:\n",
    "    img = cv2.imread(filename, 0)\n",
    "    img = cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "    pos_images[positive_files.index(filename)] = img\n",
    "  \n",
    "  # add negative images to train_image np array\n",
    "  neg_images = np.zeros((len(negative_files), IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "  for filename in negative_files:\n",
    "    img = cv2.imread(filename, 0)\n",
    "    img = cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "    neg_images[negative_files.index(filename)] = img\n",
    "  \n",
    "  train_images = np.zeros((len(positive_files)+len(negative_files), IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "  train_images = np.concatenate((pos_images, neg_images), axis=0).astype(int)\n",
    "  return train_images, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Path exists: True\n",
      "train_images:  (1480, 128, 64)\n",
      "train_labels:  (1480,)\n",
      "[[196 197 201 ... 124 119 117]\n",
      " [195 197 200 ... 125 118 115]\n",
      " [195 196 200 ... 126 116 111]\n",
      " ...\n",
      " [181 181 181 ... 182 181 181]\n",
      " [178 178 177 ... 184 184 184]\n",
      " [176 176 175 ... 186 186 186]]\n",
      "0\n",
      "[[198 198 197 ... 123 113 106]\n",
      " [196 196 196 ... 121 115 113]\n",
      " [194 194 194 ... 121 123 128]\n",
      " ...\n",
      " [247 247 246 ... 246 245 245]\n",
      " [250 250 248 ... 244 244 244]\n",
      " [247 247 246 ... 246 247 247]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "Positive_Images = 1200\n",
    "\n",
    "\n",
    "print(f\"Path exists: {os.path.isdir(positive_path) and os.path.isdir(negative_path)}\")\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_HEIGHT = 128\n",
    "R_train_images, train_labels = CreateTrainSet(positive_path, negative_path, IMAGE_WIDTH, IMAGE_HEIGHT, Positive_Images)\n",
    "\n",
    "print(\"train_images: \", R_train_images.shape)\n",
    "print(\"train_labels: \", train_labels.shape)\n",
    "print(R_train_images[0])\n",
    "print(train_labels[0])\n",
    "print(R_train_images[-1])\n",
    "print(train_labels[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns HoG features, and orderd features\n",
    "def HoG_features(images):\n",
    "    cell_size = (8,8)\n",
    "    block_size = (4,4)\n",
    "    nbins = 4\n",
    "\n",
    "    # all images have same shape\n",
    "    img_size = images[0].shape\n",
    "\n",
    "    # creating HoG object\n",
    "    hog = cv2.HOGDescriptor(_winSize=(img_size[1] // cell_size[1] * cell_size[1],\n",
    "                                    img_size[0] // cell_size[0] * cell_size[0]),\n",
    "                            _blockSize=(block_size[1] * cell_size[1],\n",
    "                                        block_size[0] * cell_size[0]),\n",
    "                            _blockStride=(cell_size[1], cell_size[0]),\n",
    "                            _cellSize=(cell_size[1], cell_size[0]),\n",
    "                            _nbins=nbins)\n",
    "\n",
    "    features = []\n",
    "    for i in range(images.shape[0]):\n",
    "        \n",
    "        # Compute HoG features\n",
    "        features.append(hog.compute((images[i]).astype(np.uint8)).reshape(1, -1))\n",
    "    \n",
    "    # Stack arrays in sequence vertically \n",
    "    features = np.vstack(features)\n",
    "   \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trained_features_reshaped:  (1480, 4160)\ntrained_features_reshaped[0]:  [0.03915166 0.0065741  0.00676362 ... 0.02321829 0.02239115 0.00087363]\n"
     ]
    }
   ],
   "source": [
    "# getting HoG features\n",
    "train_features = HoG_features(R_train_images)\n",
    "# test_features = HoG_features(R_test_images, FEATURE_SIZE)\n",
    "\n",
    "print(\"trained_features_reshaped: \", train_features.shape)\n",
    "print(\"trained_features_reshaped[0]: \", train_features[0])\n",
    "# print(\"test_features_reshaped: \", test_features.shape)\n",
    "# print(\"test_features_reshaped[0]: \", test_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NonLinear_SVM(train_features, train_labels, gamma, C, random_state=None):\n",
    "    # creating non-linear svc object, RBF kernel is default\n",
    "    clf = svm.SVC(C=C, gamma=gamma, random_state=random_state)\n",
    "\n",
    "    # fit and predict\n",
    "    clf.fit(train_features, train_labels)\n",
    "    return clf\n",
    "\n",
    "def predict(clf, test_features, test_labels):\n",
    "    predict = clf.predict(test_features)\n",
    "    \n",
    "    # using accruacy score from metrics lib and multiply 100 to get precentage\n",
    "    accuracy = accuracy_score(test_labels, predict)*100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_size: 1184\ntest_size: 296\ntrain_split: (1184, 4160) and (1184,)\nval_split: (296, 4160) and (296,)\n"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "pos_count = Positive_Images\n",
    "neg_count = 280\n",
    "pos_train_split = int(pos_count*4/k_fold)\n",
    "neg_train_split = int(pos_count+neg_count*4/k_fold)\n",
    "\n",
    "print(f\"train_size: {pos_train_split+neg_train_split-pos_count}\")\n",
    "print(f\"test_size: {pos_count-pos_train_split+neg_count-neg_train_split+pos_count}\")\n",
    "\n",
    "# splitting all pos and neg into 4/5 for train and 1/5 split for test\n",
    "train_features_split = np.concatenate((train_features[: pos_train_split],  train_features[pos_count: neg_train_split]), axis=0)\n",
    "train_labels_split = np.concatenate((train_labels[: pos_train_split], train_labels[pos_count: neg_train_split]), axis=0)\n",
    "\n",
    "val_features_split = np.concatenate((train_features[pos_train_split:pos_count],  train_features[neg_train_split:]), axis=0)\n",
    "val_labels_split = np.concatenate((train_labels[pos_train_split:pos_count],  train_labels[neg_train_split:]), axis=0)\n",
    "\n",
    "print(f\"train_split: {train_features_split.shape} and {train_labels_split.shape}\")\n",
    "print(f\"val_split: {val_features_split.shape} and {val_labels_split.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gamma: scale, C: 10,  Accuracy: 96.62%\n"
     ]
    }
   ],
   "source": [
    "# Optimal SVM Classifer\n",
    "gamma = \"scale\"\n",
    "C = 10\n",
    "Optimal_Clf = NonLinear_SVM(train_features_split, train_labels_split, gamma, C)\n",
    "accuracy = predict(clf, val_features_split, val_labels_split)\n",
    "print(f\"Gamma: {gamma}, C: {C},  Accuracy: {round(accuracy, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gamma: auto, C: 0.01,  Accuracy: 81.08%\n",
      "Gamma: auto, C: 0.1,  Accuracy: 81.08%\n",
      "Gamma: auto, C: 1,  Accuracy: 81.08%\n",
      "Gamma: auto, C: 10,  Accuracy: 87.16%\n",
      "Gamma: auto, C: 100,  Accuracy: 95.27%\n",
      "Gamma: auto, C: 1000,  Accuracy: 95.95%\n",
      "Gamma: scale, C: 0.01,  Accuracy: 81.08%\n",
      "Gamma: scale, C: 0.1,  Accuracy: 83.45%\n",
      "Gamma: scale, C: 1,  Accuracy: 95.95%\n",
      "Gamma: scale, C: 10,  Accuracy: 96.62%\n",
      "Gamma: scale, C: 100,  Accuracy: 96.62%\n",
      "Gamma: scale, C: 1000,  Accuracy: 96.62%\n",
      "Best parameters:  {'gamma': 'scale', 'C': 10, 'accuracy': 96.62}\n"
     ]
    }
   ],
   "source": [
    "MIN_ACCURACY = 50\n",
    "GammaList = ['auto', 'scale']\n",
    "C_List = [0.01, 0.1, 1, 10, 100, 1000]\n",
    "Best_SVM = {\"gamma\":None, \"C\":None, \"accuracy\":0}\n",
    "for gamma in GammaList:\n",
    "    for C in C_List:\n",
    "        clf = NonLinear_SVM(train_features_split, train_labels_split, gamma, C)\n",
    "        accuracy = predict(clf, val_features_split, val_labels_split)\n",
    "        if round(accuracy, 2) > MIN_ACCURACY:\n",
    "            print(f\"Gamma: {gamma}, C: {C},  Accuracy: {round(accuracy, 2)}%\")\n",
    "        if round(accuracy, 2) > Best_SVM[\"accuracy\"]:\n",
    "            Best_SVM[\"gamma\"] = gamma\n",
    "            Best_SVM[\"C\"] = C\n",
    "            Best_SVM[\"accuracy\"] = round(accuracy, 2)\n",
    "print(\"Best parameters: \", Best_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_SVC(train_features, train_labels, train_index, val_index, k_folds):\n",
    "    total_accuracy = 0\n",
    "    for i in range(k_folds):\n",
    "        x_train, x_val = train_features[train_index], train_features[val_index]\n",
    "        y_train, y_val = train_labels[train_index], train_labels[val_index]\n",
    "        clf = NonLinear_SVM(x_train, y_train, gamma, C)\n",
    "        total_accuracy += predict(clf, x_val, y_val)\n",
    "    avg_accuracy = total_accuracy/k_folds\n",
    "    return avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gamma: auto, C: 0.01,  Accuracy: 88.18%, time taken to train/test: 35.54\n",
      "Gamma: auto, C: 0.1,  Accuracy: 83.11%, time taken to train/test: 33.97\n",
      "Gamma: auto, C: 1,  Accuracy: 81.08%, time taken to train/test: 30.3\n",
      "Gamma: auto, C: 10,  Accuracy: 86.49%, time taken to train/test: 30.58\n",
      "Gamma: auto, C: 100,  Accuracy: 94.93%, time taken to train/test: 22.84\n",
      "Gamma: auto, C: 1000,  Accuracy: 95.95%, time taken to train/test: 17.95\n",
      "Gamma: scale, C: 0.01,  Accuracy: 77.36%, time taken to train/test: 28.71\n",
      "Gamma: scale, C: 0.1,  Accuracy: 82.09%, time taken to train/test: 32.66\n",
      "Gamma: scale, C: 1,  Accuracy: 96.28%, time taken to train/test: 27.2\n",
      "Gamma: scale, C: 10,  Accuracy: 97.97%, time taken to train/test: 29.54\n",
      "Gamma: scale, C: 100,  Accuracy: 97.64%, time taken to train/test: 28.62\n",
      "Gamma: scale, C: 1000,  Accuracy: 97.64%, time taken to train/test: 26.75\n",
      "Best parameters:  {'gamma': 'scale', 'C': 10, 'accuracy': 97.97}\n"
     ]
    }
   ],
   "source": [
    "# 5 fold cross validation dataset\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "kf.get_n_splits(train_features)\n",
    "\n",
    "MIN_ACCURACY = 50\n",
    "GammaList = ['auto', 'scale']\n",
    "C_List = [0.01, 0.1, 1, 10, 100, 1000]\n",
    "Best_SVM = {\"gamma\":None, \"C\":None, \"accuracy\":0}\n",
    "for gamma in GammaList:\n",
    "    for C in C_List:\n",
    "        start_time = time.time()\n",
    "        for train_index, val_index in kf.split(train_features):\n",
    "            accuracy = k_fold_SVC(train_features, train_labels, train_index, val_index, 5)\n",
    "        time_taken = (time.time() - (start_time))/k_folds\n",
    "        if round(accuracy, 2) > MIN_ACCURACY:\n",
    "            print(f\"Gamma: {gamma}, C: {C},  Accuracy: {round(accuracy, 2)}%, time taken to train/test: {round(time_taken, 2)}\")\n",
    "        if round(accuracy, 2) > Best_SVM[\"accuracy\"]:\n",
    "            Best_SVM[\"gamma\"] = gamma\n",
    "            Best_SVM[\"C\"] = C\n",
    "            Best_SVM[\"accuracy\"] = round(accuracy, 2)\n",
    "print(\"Best parameters: \", Best_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "detectron_file = open(path + 'detectron_count.csv')\n",
    "detectron_reader = csv.reader(detectron_file, delimiter=',')\n",
    "\n",
    "accfile = open(path + 'accuracy_comparison.csv')\n",
    "acc_writer = csv.reader(accfile, delimiter=',')\n",
    "acc_writer.writerow([\"id\", \"accuracy\"])\n",
    "\n",
    "svm_file = open(path + 'svm_submission.csv', 'w+', newline='')\n",
    "svm_writer = csv.writer(svm_file)\n",
    "svm_writer.writerow([\"id\", \"accuracy\"])\n",
    "\n",
    "line_count = 0\n",
    "for row in detectron_reader:\n",
    "        line_count += 1\n",
    "    else:\n",
    "        writer.writerow([row[0], row[1]])\n",
    "        line_count += 1\n",
    "print(f'Processed {line_count} lines.')\n",
    "\n",
    "cfile.close\n",
    "accfile.close\n",
    "svm_file.close"
   ]
  }
 ]
}